{
  "hash": "030dd8656aaa991cadd1b5d3d8f6a2aa",
  "result": {
    "markdown": "# Lineáris regresszió\n\n## Egyszerű lineáris regresszió\n\n-   Legsokrétűbb statisztikai próba\n\n-   A legtöbb más statisztikai próbát regresszióként is lehet értelmezni\n\n-   Célja: a kimeneti változó értékeinek predikciója egy (vagy több) prediktor változó által\n\n-   Adat = model + error\n\n    -   Lineáris regresszió esetében a modellünk egy egyenes vonal\n    -   A modell a bemeneti változó függvényében ad előrejelzést\n    -   Ha ismerjük a prediktor értékét, akkor meg tudjuk mondani mi lesz a kimeneti változó értéke\n\n-   Azt a lineáris modellt akarjuk megtalálni, ami legjobban illeszkedik az adatokra, ahol legkisebb a hiba\n\n### Lineáris modell paraméterei\n\n-   Másnéven regressziós **együtthatók (regression coefficients)**\n\n-   **Meredekség (slope):** egy egységnyi változás a prediktor változóban (x tengely) mekkora változást okoz a kimeneti változóban (y tengely)\n\n    -   Az egyenes dőlésszöge\n\n    -   Ha pozitív érték, akkor pozitív kapcsolat van a két változó között\n\n        -   Ha a prediktor változónk értéke nő a kimeneti változó értéke is nő\n\n    -   Ha negatív érték, akkor negatív kapcsolat\n\n        -   Ha a prediktor változónk értéke nő, a kimeneti változónk értéke csökken\n\n    -   Jele: b~1~\n\n-   **Intercept:** ha a prediktor változó értéke 0, mekkora a kimeneti változó értéke, azaz milyen y értéknél metszi a vonal az y tengelyt\n\n    -   Jele: b~0~\n\n-   **Hibatag:** a modell által meg nem magyarázott variancia\n\n    -   Az adatoponttól a lineáris regressziós egyenesig húzott szaggatott vonal\n\n### Lineáris modell illeszkedésének a vizsgálata\n\n-   Hogyan találjuk meg azt a lineáris modellt, ami legjobban illeszkedik az adatokra?\n\n#### A **legkisebb négyzetek módszere (method of least squares)**\n\n-   Meghatározásához ugyanazt a módszert használhatjuk, mint amikor az *átlag modellnél* használtunk\n\n-   Megnézzük a modell által prediktált értékek és a valós értékek közötti különbséget: a **reziduálisokat (residuals)**\n\n-   Itt is négyzetre emeljük a reziduálisok majd összeadjuk őket (sum of squared differences, SS)\n\n-   Ezután minden lehetséges vonalra (lineáris regressziós modellre) kiszámíthatnánk őket és ahol az SS a legkisebb az a modell illeszkedik a legjobban az adatokra\n\n-   Azonban még a legjobban illeszkedő modell is magyarázhatja rosszul az adatokat!\n\n#### A null modell\n\n-   Ahhoz, hogy ezt megvizsgálájuk a **legegyszerűbb (null) modell** illeszkedéséhez hasonlítjuk a regressziós modellunk illeszkedését: az átlaghoz\n\n    -   Az átlag azonban minden adatpontra ugyanazt az értéket fogja prediktálni\n\n        -   Például: mennyire számít a marketingre szánt összeg egy film összbevételénél?\n\n            -   Ha 1 dollárt költünk a marketingre akkor is ugyanazt a bevétlet prediktálja, mintha 200000\\$ költüttünk volna rá\n\n    -   Az átlag modellnél a hibát összesítve megkapjuk a **teljes négyzetösszeget (total sum of squares, SS~T~)**\n\n        -   Miért az átlag a legegyszerűbb modell?\n            -   Bármelyik másik értéket választjuk a legegyszerűbb modellhez, a hibák négyzetösszege (sum of squared residuals) minden esetben nagyobb lesz, mintha az átlag értékét választanánk\n\n#### Regressziós modell összehasonlítása a null modellel\n\n-   Kiszámítjuk a **négyzetre emelt reziduálisok összegét** a regressziós modellnél is **(residual sum of squares, SS~R~)**\n\n-   Ahhoz, hogy megtudjuk a regressziós modellünk mennyivel jobban magyarázza az adatokat, mint az átlag modellunk a kettőt kivonjuk egymásból: SS~T~ - SS~R~ = **SS~M~ (model sum of squares)**\n\n    -   Úgy tudunk javítani a modellünkön, ha közelebb kerülnek a modell által prediktált értékek az egyes adatpontokhoz (lásd: @fig-ssm)\n\n        -   Így csökken a modell által prediktált értékek és az adatok közti különbség, tehát a reziduálisok nagysága\n\n    -   Ha az SS~M~ nagy, a regressziós modell sokkal jobban magyarázza az adatokat, mint az átlag\n\n    -   A fennmaradó variancia (amit a modell nem tud megmagyarázni) a **megmagyarázatlan variancia**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![ A bal oldalon lévő ábrán látható statisztikai modell (piros vonal) az átlag. A fekete pontok az egyes adatpontokat ábrázolják. Az átlaghoz tartozó modell négyzetes hibáinak összege 9.05. Ezzel szemben a jobb oldalon a statisztikai modell egy lineáris regressziós egyenes. Ennek a hiba értéke kisebb, 5.43. Tehát a lineáris regressziós modell jobban illeszkedik az adatokra. Nagy Tamás ábrája.](figures/regression/ssm.png){#fig-ssm fig-align='center' width=70%}\n:::\n:::\n\n\n-   Megnézhetjük, hogy arányosan mennyivel javul a modellünk az átlaghoz képest, ha egy prediktor változót is belerakunk\n\n    -   A kettőt elosztva egymással és kivonva egyből megkapjuk a **determinisztikus együtthatót R^2^**\n\n        -   1 - SS~M~ / SS~T~\n\n    -   Megmutatja a regressziós modellünk által megmagyarázott variancia arányát a kimeneti változónkban, a teljes varianciához képest (lásd: @fig-rsquare)\n\n    -   0 és 1 közötti értéket vehet fel\n\n    -   Minél nagyobb az érték a modellünk az adatokban található variancia annál nagyobb százalékát magyarázza meg, tehát annál jobb\n\n    -   Ha megszorozzuk 100-al százalékot kapunk\n\n    -   A kimeneti változóban lévő varianciának hány százalékát magyarázza meg a modell\n\n    -   Ha ennek a négyzetgyökét vesszük, akkor megkapjuk a Pearson korrelációs együtthatót!\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![ A felső a függőleges szaggatott vonalok a regressziós modell által meg nem magyarázott varianciát mutatják. Az alsó ábrán a szaggatott vonalak pedig a teljes varianciát, azaz az átlag és a megfigyelt értékek közötti különbséget. Ha a kettőt elosztjuk egymással és kivonjuk egyből, akkor megkapjuk a determinisztikus együtthatót. Nagy Tamás ábrája.](figures/regression/rsquare.png){#fig-rsquare fig-align='center' width=70%}\n:::\n:::\n\n\n-   Az F-teszttel is megvizsgálhatjuk a modellünk illeszkedését\n\n    -   **F teszt statisztika**: szisztematikus variancia / nem szisztematikus variancia\n\n        -   A modell okozta javulás (SS~M~) / a modell és a megfigyel adatok között lévő különbség (SS~R~)\n\n    -   Másszóval az F teszt statisztika megmondja, hogy a modell mennyire javítja a becslésünk pontosságát a modellben található pontatlansághoz képest\n\n    -   Itt nem a négyzetes különbségek összegével, hanem átlagával dolgozunk\n\n        -   Így nem függ a megfigyelések számától\n\n    -   Mean squares for the model (MSS)\n\n        -   Szabadságfok: prediktor változók száma\n\n    -   Residuals mean square (MS~R~)\n\n        -   Szabadságfok: megfigyelések száma - béta együtthatók száma (meredség + intercept = 2)\n\n    -   Jó modellnél 1-nél nagyobb az F arány\n\n    -   p-értéket vagy konfidencia intervallumot is ki tudunk hozzá számolni\n\n### Prediktor változók szignifikanciájának vizsgálata\n\n-   Nemcsak a teljes modell teljesítményét kell megvizsgálnunk, hanem az egyes paramétereknek a szignifikanciáját is\n\n-   A béta megmutatja, hogy a prediktorban való egy egységnyi változás mekkora változást okoz a kimeneti változóban\n\n    -   Ha a modell rossz, azt várjuk el, hogy ez nulla legyen\n\n        -   Pont, mint az átlagnál!\n\n-   A nullhipotézis a paraméterek esetén az lesz, h a paraméter nem különbözik a nullától\n\n-   A kritikus érték pedig a paraméter tényleges értéke\n\n-   Ezekre gyakorlatilag egy egymintás t-próbát fogunk végezni\n\n    -   t = béta / SE\n\n-   Az intercept esetén, hogy különbözik-e a nullától az érték.\n\n-   A slope esetén, hogy a dőlésszöge különbözik-e a nullától.\n\n## Lineáris regresszió előfeltételei\n\n-   A kimeneti változó folytonos azaz legalább intervallum mérési szintű\n\n-   Prediktor típusok: folytonos vagy kategorikus is lehet\n\n-   Nem zéró variancia: a kimeneti változó és prediktor értékeiben van variabilitás\n\n-   A megfigyelések egymástól függetlenek\n\n-   A reziduálisok eloszlása normális (a prediktor eloszlásának nem kell normálisnak lennie!)\n\n    -   Vizuálisan: \n\n        -   **QQ plot**\n\n            -   A pontok maradjanak az átló közelében\n\n                -   Az esetek 5%-a lehet 2 szóráson kívül\n\n                -   Az esetek 1%-a lehet 2.5 szóráson kívül\n\n                -   Az esetek 0.1%-a lehet 3 szóráson kívül\n\n            -   A @fig-qq ábrán egy olyan QQ plot látható, ami azt mutatja, hogy a vizsgált regressziós modell esetében a reziduálisok eloszlása eltér a normálistól\n\n        -   **Residual vs fitted values**\n\n            -   Residuális értékek vannak az y tengelyen\n\n            -   Modell értékek az x tengelyen\n\n            -   Ha a vonal görbül, nem lineáris kapcsolat\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Ha a reziduálisok eloszlása a normális eloszlást követné, akkor az áblán látható adatpontok a szaggatott vonal mentén helyezkednének el. Nagy Tamás ábrája.](figures/regression/qq.png){#fig-qq fig-align='center' width=70%}\n:::\n:::\n\n\n-   Az értékek\n\n    -   68%-a egy szóráson belül van\n\n    -   95%-a két szóráson belül van\n\n    -   99.7%-a három szóráson belül van\n\n-   Ehhez kapcsolódik, hogy a modellben nincsen sok jelentős kiugró érték (outlier), ami torzítja a modellünket\n\n-   Azt várjuk, hogy a lineáris regresszió minden mérési szinten ugyanannyira jó predikciót tudjon adni. Azaz, ugyanolyan hatékony legyen akkor, ha a buszmegállóban 3 ember van, mint akkor, ha 20\n\n-   Ezt a reziduálisok elemzésével tudjuk ellenőrizni\n\n-   Ekkor azt mondjuk, hogy a modellünk homoszkedasztikus, azaz a reziduálisok mértéke független a prediktor értékétől.\n\n-   Ellentéte a heteroszkedaszticitás, ami azt jelenti, hogy pl. a kisebb prediktált értékekhez tartozó reziduálisok kisebbek, mint a nagyobb prediktált értékekhez tartozó reziduálisok\n\n-   Vizsgálata vizuálisan zajlik\n\n    -   Tölcsér alak azt jelenti hogy sérül a heteroszkedaszticitás feltétle (lásd: @fig-heteroscedasticity)\n\n-   A kimeneti vagy prediktor változók transzformálása segíthet a reziduálisok homoszkedaszticitásának sérülése során. Ilyen lehet például a természetes logaritmikus transzformáció\n\n    -   Ilyenkor figyelnünk kell az eredmények értelmezésére, hiszen a változók transzformálásával az is megváltozik\n\n        -   Ha a prediktornak vesszük természetes logarimtusát: 1%-os változás a prediktor változóban a kimeneti változó 0.01\\*B~1~ változásához vezet\n\n        -   Ha a prediktornak és a kimeneti változónak is a természetes logaritmusát vesszük: 1%-os változás a prediktor változóban B~1~%-os változáshoz vezet a kimeneti változóban\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A reziduálisok vizsgálata során az X tengelyen a modell által prediktált értékeket az Y tengelyen a standardizált reziduálisokkal szemben ábrázoljuk. Véletlen szóródást várunk el az adatokban az X tengely összes értékéhez. Az ábrán a tölcsér alakzat heteroszkedaszticitásra utal, tehát a reziduálisok függnek a prediktor értékeitől. Nagy Tamás ábrája.](figures/regression/heteroscedasticity.png){#fig-heteroscedasticity fig-align='center' width=70%}\n:::\n:::\n\n\n-   Kiugró értékek szűrése\n\n    -   A kiugró értékek \"magukhoz húzzák\" a regressziós egyenest (lásd: @fig-outlier)\n\n    -   Vizuálisan\n\n        -   Távol esnek a többi értéktől\n\n        -   Magukhoz húzzák a regressziós egyenest\n\n    -   Statisztikai módszerekkel\n\n        -   Cook's distance\n\n        -   Ha 1-nél nagyobb erős torzító hatása van az adatpontnak\n\n    -   Mit tegyünk ha vannak outlierek?\n\n        -   Csak akkor zárjuk ki ha adathibából származnak\n\n        -   Különben overfitting veszélye fennáll\n\n    -   Nagy elemszánál nem olyan nagy a hatásuk!\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Az ábrán az outlierek torzító hatása látható a regressziós modellre. A jobb felső sarokban a többi adatponttól távol egy kiugró érték látható, ez húzza magához a pirossal jelölt regressziós modellt. Nagy Tamás ábrája.](figures/regression/rsquare.png){#fig-outlier fig-align='center' width=70%}\n:::\n:::\n\n\n## Többszörös lineáris regresszió\n\n-   További tényezőkről is gondolhatjuk, hogy javítani fognak a modellünkön\n\n    -   Egy bizonyos pont után, ha ezeket a prediktor változókat hozzáadjuk a modellhez, nem fog a hiba szignifikánsan csökkenni\n\n    -   Ez az overfitting\n\n    -   Eredménye: a modell nem lesz generalizálható\n",
    "supporting": [
      "regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}